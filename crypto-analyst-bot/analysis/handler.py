# analysis/handler.py
# –ú–æ–¥—É–ª—å –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–∫–µ–Ω–æ–≤, –≤–∫–ª—é—á–∞—è –ø–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π.

import logging
import json
import os
import httpx
import time
from dotenv import load_dotenv
from telegram import Update, constants
from telegram.ext import CallbackContext
from sqlalchemy.ext.asyncio import AsyncSession

from database import operations as db_ops

logger = logging.getLogger(__name__)
load_dotenv()

GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
GEMINI_API_URL = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key={GEMINI_API_KEY}"

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_API_URL = "https://api.openai.com/v1/chat/completions"
GPT4_MODEL = os.getenv("OPENAI_GPT_MODEL", "gpt-4o")

# –∫—ç—à –æ—Ç–≤–µ—Ç–æ–≤ {payload: (timestamp, message)}
analysis_cache: dict[str, tuple[float, str]] = {}
CACHE_TTL = 600  # seconds


ANALYSIS_PROMPT = """
–¢—ã –æ–ø—ã—Ç–Ω—ã–π –∫—Ä–∏–ø—Ç–æ–∞–Ω–∞–ª–∏—Ç–∏–∫. –ò—Å–ø–æ–ª—å–∑—É–π –¥–∞–Ω–Ω—ã–µ –ø–æ–∏—Å–∫–∞ –Ω–∏–∂–µ, —á—Ç–æ–±—ã –∫—Ä–∞—Ç–∫–æ (2-3 –∞–±–∑–∞—Ü–∞) –ø–æ–¥–≤–µ—Å—Ç–∏ –∏—Ç–æ–≥ –ø–æ –∑–∞–ø—Ä–æ—Å—É.
–°—Ç—Ä—É–∫—Ç—É—Ä–∞: –≤–∞–∂–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏, –æ–±—â–∏–π —Å–µ–Ω—Ç–∏–º–µ–Ω—Ç, –≤—ã–≤–æ–¥. –ù–µ –¥–∞–≤–∞–π —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö —Å–æ–≤–µ—Ç–æ–≤. –û—Ç–≤–µ—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º –≤ Markdown.

–ó–∞–ø—Ä–æ—Å: "{user_query}"
–î–∞–Ω–Ω—ã–µ –ø–æ–∏—Å–∫–∞:
```json
{search_results}
```
–°–≤–æ–¥–∫–∞:
"""


async def _generate_summary(prompt: str) -> str:
    """–ó–∞–ø—Ä–∞—à–∏–≤–∞–µ—Ç AI-–º–æ–¥–µ–ª—å (OpenAI GPT-4o –∏–ª–∏ Gemini) –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ—Ç–≤–µ—Ç."""
    if OPENAI_API_KEY:
        headers = {
            "Authorization": f"Bearer {OPENAI_API_KEY}",
            "Content-Type": "application/json",
        }
        payload = {
            "model": GPT4_MODEL,
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.5,
        }
        async with httpx.AsyncClient(timeout=40.0) as client:
            resp = await client.post(OPENAI_API_URL, json=payload, headers=headers)
            resp.raise_for_status()
            data = resp.json()
            return data["choices"][0]["message"]["content"].strip()
    else:
        headers = {"Content-Type": "application/json"}
        api_payload = {
            "contents": [{"parts": [{"text": prompt}]}],
            "generationConfig": {"temperature": 0.5, "maxOutputTokens": 2048},
        }
        async with httpx.AsyncClient(timeout=40.0) as client:
            response = await client.post(GEMINI_API_URL, json=api_payload, headers=headers)
            response.raise_for_status()
            data = response.json()
            return data["candidates"][0]["content"]["parts"][0]["text"].strip()

async def handle_token_analysis(update: Update, context: CallbackContext, payload: str, db_session: AsyncSession):
    if not update.effective_message: return
    user_id = update.effective_user.id
    
    # --- –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ payload –Ω–µ –ø—É—Å—Ç–æ–π ---
    if not payload:
        await update.effective_message.reply_text("üòï –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —É—Ç–æ—á–Ω–∏—Ç–µ, –æ –∫–∞–∫–æ–º —Ç–æ–∫–µ–Ω–µ –≤—ã —Ö–æ—Ç–∏—Ç–µ —É–∑–Ω–∞—Ç—å.")
        return

    cache_key = payload.lower().strip()
    cached = analysis_cache.get(cache_key)
    if cached and time.time() - cached[0] < CACHE_TTL:
        final_message = cached[1]
        await update.effective_message.reply_text(final_message, parse_mode=constants.ParseMode.MARKDOWN, disable_web_page_preview=True)
        await db_ops.add_chat_message(session=db_session, user_id=user_id, role='model', text=final_message)
        return

    query = f"–Ω–æ–≤–æ—Å—Ç–∏ –∏ –∞–Ω–∞–ª–∏—Ç–∏–∫–∞ –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç—ã {payload}"
    await update.effective_message.reply_text(
        f"üîç –ò—â—É –∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø–æ –∑–∞–ø—Ä–æ—Å—É: *{payload}*. –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –¥–æ 30 —Å–µ–∫—É–Ω–¥...",
        parse_mode=constants.ParseMode.MARKDOWN,
    )
    
    try:
        search_results_list = google_search.search(queries=[query])
        
        if not search_results_list or not search_results_list[0].results:
            await update.effective_message.reply_text("üòï –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –∞–∫—Ç—É–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É.")
            return

        search_results = search_results_list[0].results
        prompt = ANALYSIS_PROMPT.format(
            user_query=payload,
            search_results=json.dumps(search_results, indent=2, ensure_ascii=False),
        )

        analysis_text = await _generate_summary(prompt)

        final_message = f"üìä *–ê–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∞—è —Å–≤–æ–¥–∫–∞ –ø–æ {payload}*\n\n{analysis_text}"
        await update.effective_message.reply_text(
            final_message,
            parse_mode=constants.ParseMode.MARKDOWN,
            disable_web_page_preview=True,
        )
        await db_ops.add_chat_message(session=db_session, user_id=user_id, role='model', text=final_message)
        analysis_cache[cache_key] = (time.time(), final_message)

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–∫–µ–Ω–∞: {e}", exc_info=True)
        await update.effective_message.reply_text("üí• –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è —Å–±–æ—Ä–∞ –∏ –∞–Ω–∞–ª–∏–∑–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.")